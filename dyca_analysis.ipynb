{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17a5b14",
   "metadata": {},
   "source": [
    "# This Code does the DyCA Analysis of the sEEG Data from EBRAINS (Amir)\n",
    "\n",
    "- First try to handle the data in the propper manner (xarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dyca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98271d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = xr.load_dataarray(\"/home/astiehl/05_Code/Promotion/marseille/IREM/seeg_epochs/HID-Sub-001_MVIS_MVEB_Epochs.nc\")\n",
    "# count nans and notnans in the data\n",
    "nans = np.isnan(test_array).sum().item()\n",
    "notnans = np.count_nonzero(~np.isnan(test_array))\n",
    "\n",
    "print(f\"Number of NaNs in the data array: {nans}\")\n",
    "print(f\"Number of non-NaNs in the data array: {notnans}\")\n",
    "\n",
    "# maybe i can reconstruct the number of datapoints\n",
    "total_points = nans + notnans\n",
    "total_points2 = test_array.size\n",
    "total_points3 = 256*93*2*2*2*36\n",
    "print(f\"Total number of data points (nans):n): {total_points}\")\n",
    "print(f\"Total number of data points (size): {total_points2}\")\n",
    "print(f\"Total number of data points (manual calc): {total_points3}\")\n",
    "# JUHU - thats the same and I understood the data structure\n",
    "\n",
    "# Where happens the NaNs?\n",
    "# Get unique combinations of task, band, event, trial and region where NaNs are located (excluding time)\n",
    "nan_mask = np.isnan(test_array)\n",
    "# Check if any time point has NaN for each combination of other dimensions\n",
    "nan_combinations = nan_mask.any(dim='time')\n",
    "nan_indices = np.argwhere(nan_combinations.values)\n",
    "\n",
    "unique_combinations = set()\n",
    "for loc in nan_indices:\n",
    "    task, band, event, trial, region = loc\n",
    "    combination = (task, band, event, trial, region)\n",
    "    unique_combinations.add(combination)\n",
    "\n",
    "i = 0\n",
    "for task, band, event, trial, region in sorted(unique_combinations):\n",
    "    print(f\"{i}. NaN found at - Task: {test_array.task[task].item()}, Band: {test_array.band[band].item()}, Event: {test_array.event[event].item()}, Trial: {test_array.trial[trial].item()}, Region: {test_array.region[region].item()}\")\n",
    "    i += 1\n",
    "# Okay, when a NaN is present, it is present for all timepoints in that combination of other dimensions\n",
    "\n",
    "# Print unique values for each dimension where NaNs are located\n",
    "unique_tasks = set()\n",
    "unique_bands = set()\n",
    "unique_events = set()\n",
    "unique_trials = set()\n",
    "unique_regions = set()\n",
    "\n",
    "for task, band, event, trial, region in unique_combinations:\n",
    "    unique_tasks.add(test_array.task[task].item())\n",
    "    unique_bands.add(test_array.band[band].item())\n",
    "    unique_events.add(test_array.event[event].item())\n",
    "    unique_trials.add(test_array.trial[trial].item())\n",
    "    unique_regions.add(test_array.region[region].item())\n",
    "\n",
    "print(\"\\nUnique values where NaNs are found:\")\n",
    "print(f\"Tasks: {sorted(unique_tasks)}\")\n",
    "print(f\"Bands: {sorted(unique_bands)}\")\n",
    "print(f\"Events: {sorted(unique_events)}\")\n",
    "print(f\"Trials: {sorted(unique_trials)}\")\n",
    "print(f\"Regions: {sorted(unique_regions)}\")\n",
    "# here you can see, that all the Nans happens in the Trial 35 \n",
    "    \n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912b5d1",
   "metadata": {},
   "source": [
    "## Ready with playing with the data - Let's start some analysis\n",
    "\n",
    "- group the data accordingly:\n",
    "    - task, band, event, feedback\n",
    "- let's run some dyca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete trial 35\n",
    "cleaned_array = test_array.sel(trial=test_array.trial != 35)\n",
    "\n",
    "# count nans and notnans in the cleaned data\n",
    "nans_cleaned = np.isnan(cleaned_array).sum().item()\n",
    "notnans_cleaned = np.count_nonzero(~np.isnan(cleaned_array))\n",
    "print(f\"Number of NaNs in the cleaned data array: {nans_cleaned}\")\n",
    "print(f\"Number of non-NaNs in the cleaned data array: {notnans_cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the regions \n",
    "print(cleaned_array.region.values)\n",
    "unique_regions = set(cleaned_array.region.values)\n",
    "print(unique_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e65071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the rank of every single trial\n",
    "ranks = []\n",
    "for trial in cleaned_array.trial.values:\n",
    "    trial_data = cleaned_array.sel(trial=trial)\n",
    "    for i in range(trial_data.shape[0]): # task\n",
    "        for j in range(trial_data.shape[1]): # band\n",
    "            for k in range(trial_data.shape[2]): # event\n",
    "                region_time_matrix = trial_data[i, j, k, :, :].values # shape (93, 256)\n",
    "                if np.isnan(region_time_matrix).all():\n",
    "                    rank = 0\n",
    "                else:\n",
    "                    rank = np.linalg.matrix_rank(np.nan_to_num(region_time_matrix))\n",
    "                ranks.append((trial, cleaned_array.task[i].item(), cleaned_array.band[j].item(), cleaned_array.event[k].item(), rank))\n",
    "                \n",
    "ranks_df = pd.DataFrame(ranks, columns=['trial', 'task', 'band', 'event', 'rank'])\n",
    "print(ranks_df)\n",
    "\n",
    "# print the histogramm of the ranks\n",
    "unique, counts = np.unique(ranks_df['rank'], return_counts=True)\n",
    "rank_histogram = dict(zip(unique, counts))\n",
    "print(\"\\nHistogram of ranks:\")\n",
    "for rank, count in sorted(rank_histogram.items()):\n",
    "    print(f\"Rank {rank}: {count} occurrences\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd27cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d122ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by task, band, event, feedback (so we have at the end 24 arrays)\n",
    "# Get unique values for each dimension\n",
    "tasks = cleaned_array.task.values\n",
    "bands = cleaned_array.band.values\n",
    "events = cleaned_array.event.values\n",
    "\n",
    "print(f\"Tasks: {tasks}\")\n",
    "print(f\"Bands: {bands}\")\n",
    "print(f\"Events: {events}\")\n",
    "print(f\"Shape: {cleaned_array.shape}\")\n",
    "\n",
    "\n",
    "# Create grouped arrays for each combination\n",
    "grouped_arrays = {}\n",
    "\n",
    "for task in tasks:\n",
    "    for band in bands:\n",
    "        for event in events:\n",
    "                group_name = f\"task_{task}_band_{band}_event_{event}\"\n",
    "                \n",
    "                # Select data for this specific combination\n",
    "                group_data = cleaned_array.sel(\n",
    "                    task=task,\n",
    "                    band=band,\n",
    "                    event=event\n",
    "                )\n",
    "                \n",
    "                grouped_arrays[group_name] = group_data\n",
    "                print(f\"Created group: {group_name}, shape: {group_data.shape}\")\n",
    "\n",
    "print(f\"\\nTotal number of groups created: {len(grouped_arrays)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5526fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean over the trials for each group\n",
    "mean_grouped_arrays_trails = {}\n",
    "for group_name, group_data in grouped_arrays.items():\n",
    "    mean_data = group_data.mean(dim='trial')\n",
    "    mean_grouped_arrays_trails[group_name] = mean_data\n",
    "    print(f\"Calculated mean for group: {group_name}, shape: {mean_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rank for each mean group\n",
    "ranked_grouped_arrays = {}\n",
    "for group_name, mean_data in mean_grouped_arrays_trails.items():\n",
    "    rank_of_data = np.linalg.matrix_rank(mean_data)\n",
    "    ranked_grouped_arrays[group_name] = rank_of_data\n",
    "    print(f\"Calculated rank for group: {group_name}, rank: {rank_of_data}, full shape: {mean_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d76c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  This code works technically, but it is not very useful, because there are no non_variance channels nor duplicate channels\n",
    "\n",
    "# def find_duplicate_channels(X, channel_names=None, tol=1e-8):\n",
    "#     if channel_names is None:\n",
    "#         channel_names = [f\"ch{i}\" for i in range(X.shape[0])]\n",
    "\n",
    "#     corr = np.corrcoef(X)\n",
    "#     dup_pairs = []\n",
    "#     n = len(channel_names)\n",
    "#     for i in range(n):\n",
    "#         for j in range(i+1, n):\n",
    "#             if np.abs(corr[i, j] - 1.0) < tol:\n",
    "#                 dup_pairs.append((channel_names[i], channel_names[j]))\n",
    "#     return dup_pairs\n",
    "\n",
    "# # if the rank is not full, we need to delete the double regions\n",
    "# cleaned_ranked_grouped_arrays = {}\n",
    "# threshold_var = 1e-12\n",
    "# for group_name, rank in ranked_grouped_arrays.items():\n",
    "#     # change the region names to: regionname_1, regionname_2, ...\n",
    "#     region_names = [f\"{name}_{i}\" for i, name in enumerate(mean_grouped_arrays[group_name].region.values)]\n",
    "#     mean_data = mean_grouped_arrays[group_name]\n",
    "#     full_rank = min(mean_data.sizes['region'], mean_data.sizes['time'])\n",
    "#     if rank < full_rank:\n",
    "#         print(f\"Group {group_name} has rank {rank} which is less than full rank {full_rank}. Cleaning...\")\n",
    "#         # find channels with low variance\n",
    "#         var = np.var(mean_data.values, axis = 1)\n",
    "#         mask_var = var > threshold_var\n",
    "#         cleaned_data = mean_data.values[mask_var, :]\n",
    "#         cleaned_rank = np.linalg.matrix_rank(cleaned_data)\n",
    "#         print(f\"After cleaning (low variance), new shape: {cleaned_data.shape}, new rank: {cleaned_rank}\")\n",
    "#         # find duplicate channels  \n",
    "#         cleaned_region_names = [region_names[i] for i in range(len(region_names)) if mask_var[i]]\n",
    "#         dup_channels = find_duplicate_channels(cleaned_data, channel_names=cleaned_region_names, tol=1e-1)\n",
    "#         print(f\"Duplicate channels found: {dup_channels}\")\n",
    "#         print(f\"How many duplicate channels: {len(dup_channels)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa97dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other idea: calculate the mean of every region over time (because some regions are the same, but from different electrodes)\n",
    "# then calculate the rank of this mean matrix (regions x mean over time)\n",
    "mean_over_regions_grouped_arrays = {}\n",
    "\n",
    "for group_name, group_data in mean_grouped_arrays_trails.items():\n",
    "    mean_over_regions_grouped_arrays[group_name] = {}\n",
    "    # falls region Koordinate ist:\n",
    "    unique_regions = np.unique(group_data['region'].values)\n",
    "\n",
    "    for unique_region in unique_regions:\n",
    "        region_sel = group_data.sel(region=unique_region)\n",
    "        if len(region_sel.shape) == 1:\n",
    "            # extent the shape\n",
    "            region_sel = np.expand_dims(region_sel, axis=0)\n",
    "            # print(region_sel.shape)\n",
    "        region_mean = np.mean(region_sel, axis=0)  # mean over region\n",
    "        if len(region_mean.shape) == 0:\n",
    "            print(region_mean.shape)\n",
    "            print(region_sel.shape)\n",
    "        # print(region_mean.shape)\n",
    "        mean_over_regions_grouped_arrays[group_name][unique_region] = region_mean\n",
    "mean_over_regions_grouped_arrays[group_name].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the rank of each group\n",
    "ranked_mean_over_regions_grouped_arrays = {}\n",
    "for group_name, region_dict in mean_over_regions_grouped_arrays.items():\n",
    "    region_means_matrix = np.array(list(region_dict.values()))  # shape (n_regions, n_timepoints)\n",
    "    rank_of_data = np.linalg.matrix_rank(region_means_matrix)\n",
    "    ranked_mean_over_regions_grouped_arrays[group_name] = rank_of_data\n",
    "    print(f\"Calculated rank for group: {group_name}, rank: {rank_of_data}, full shape: {region_means_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c82c5e",
   "metadata": {},
   "source": [
    "The Variable `mean_over_regions_grouped_arrays` contains now a grouped array, full rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c05774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dyca on the first trial of each group\n",
    "dyca_results = {}\n",
    "for group_name, region_dict in mean_over_regions_grouped_arrays.items():\n",
    "    data = np.array(list(region_dict.values()))  # shape (n_regions, n_timepoints)    \n",
    "    print(f\"Running dyca on group: {group_name}, data shape: {data.shape}\")\n",
    "    \n",
    "    # Run dyca\n",
    "    time = np.linspace(0, 1, 256)\n",
    "    dyca_result = dyca.dyca(data.T, time)\n",
    "    dyca_results[group_name] = dyca_result\n",
    "    print(f\"dyca completed for group: {group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20824cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dyca eigenvalues for each group\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "\n",
    "for group_name, dyca_result in dyca_results.items():\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(dyca_result['generalized_eigenvalues'])), dyca_result['generalized_eigenvalues'])\n",
    "    plt.title(f\"DyCA Eigenvalues for {group_name}\")\n",
    "    plt.xlabel(\"Component\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
